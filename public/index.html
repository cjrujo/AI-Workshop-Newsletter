<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>ai-workshop-newsletter</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="">
    <meta name="generator" content="Hugo 0.123.6">
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    

    
      

    

    
    
      <link href="/index.xml" rel="alternate" type="application/rss+xml" title="ai-workshop-newsletter" />
      <link href="/index.xml" rel="feed" type="application/rss+xml" title="ai-workshop-newsletter" />
      
    

    
      <link rel="canonical" href="https://ai-workshop-newsletter.pages.dev/">
    

    <meta property="og:title" content="ai-workshop-newsletter" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://ai-workshop-newsletter.pages.dev/" />

<meta itemprop="name" content="ai-workshop-newsletter">
<meta itemprop="description" content=""><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="ai-workshop-newsletter"/>
<meta name="twitter:description" content=""/>

	
  </head>

  <body class="ma0 avenir bg-near-white">

    

  <header>
    <div class="pb3-m pb6-l bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        ai-workshop-newsletter
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/posts/" title="Posts page">
              Posts
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/tutorials/" title="Tutorials page">
              Tutorials
            </a>
          </li>
          
        </ul>
      
      
<div class="ananke-socials">
  
</div>

    </div>
  </div>
</nav>

      <div class="tc-l pv3 ph3 ph4-ns">
        <h1 class="f2 f-subheadline-l fw2 light-silver mb0 lh-title">
          ai-workshop-newsletter
        </h1>
        
      </div>
    </div>
  </header>


    <main class="pb7" role="main">
      
 <article class="cf ph3 ph5-l pv3 pv4-l f4 tc-l center measure-wide lh-copy mid-gray">
    
  </article>
  
  
  
  
  
  
  
    
    

    <div class="pa3 pa4-ns w-100 w-70-ns center">
      
       
          <h1 class="flex-none">
            Recent Tutorials
          </h1>
        

      

      <section class="w-100 mw8">
        
        
          <div class="relative w-100 mb4">
            
<article class="bb b--black-10">
  <div class="db pv4 ph3 ph0-l no-underline dark-gray">
    <div class="flex flex-column flex-row-ns">
      
      <div class="blah w-100">
        <h1 class="f3 fw1 athelas mt0 lh-title">
          <a href="/tutorials/1/01/01/" class="color-inherit dim link">
            
            </a>
        </h1>
        <div class="f6 f5-l lh-copy nested-copy-line-height nested-links">
          <!DOCTYPE html>000_Table_of_Content000_Table_of_Content (continuous update)¶Plan¶001 Survival kit¶002 Machine &amp; Deep learning¶003 LLM - Basic¶004 LLM - Prompt engineering¶005 LLM - Content Generation¶006 LLM - API¶007 LLM - RAG¶008 Local LLM¶009 AI Agent¶010 Applications¶
        </div>
          <a href="/tutorials/1/01/01/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
        
      </div>
    </div>
  </div>
</article>

          </div>
        
          <div class="relative w-100 mb4">
            
<article class="bb b--black-10">
  <div class="db pv4 ph3 ph0-l no-underline dark-gray">
    <div class="flex flex-column flex-row-ns">
      
      <div class="blah w-100">
        <h1 class="f3 fw1 athelas mt0 lh-title">
          <a href="/tutorials/1/01/01/" class="color-inherit dim link">
            
            </a>
        </h1>
        <div class="f6 f5-l lh-copy nested-copy-line-height nested-links">
          <!DOCTYPE html>002_Machine_&amp;_Deep_Learning002 Machine &amp; Deep learning¶本文简单描述机器学习和深度学习，让读者了解机器学习和深度学习的基本概念。进一步的理论学习，可参考以下两位教授吴恩达及李宏毅教学资料，本文图片也引用课程资料。
http://www.ai-start.com/ml2014
https://speech.ee.ntu.edu.tw/~hylee/index.php
什么是机器学习¶人类学习 $y=f(x)$人类学习所解决的问题是，已知 $x$，透过一个数学计算方程式（模型） $f$， 计算未知数 $y$ 的过程。机器学习 $y=f(x)$机器学习所解决的问题是，已知 $x$, $y$, 透过计算机迭代一个数学模型 $f$, 使 $f(x)$ 最接近 $y$的过程。简言之，人类学习过程中学习推理出来的数学模型，利用已知数和数学模型“求解未知数”。而机器学习则是透过已知输入与输出，“寻找一个数学模型”，使这个模型能处理相似的问题。以简单的线性方程式来表示，$y=b+w*x$,其中${x,y}$分别是模型训练所需要的训练资料集中已知的输入和输出，${b,w}$则为bias（偏差）和weighting（权重），以二维线性方程式的物理意义，bias控制着直线的高度位置，weighting控制着直线的斜率，模型训练的目的，在于找出一组${b,w}$使测试资料集的$x$经过模型的计算，所得到的$\hat{y}$最接近于测试资料集中$x$所对应的$y$。机器如何学习（找出未知的方程式）¶综上节所述,机器学习,需要具备以下条件:
已知的数据，通常称为training data，为了验证训练后模型是否可靠，通常会把已知的数据分为training data和test data两部分，第一部分用来训练模型，第二部分用来验证模型。训练的模型及其参数，前述简单的线性方程式$y=b+w*x$，在模拟现实世界的问题时，会加上非线性函数，成为激活函数(activate function)，使模型更能贴近生活中绝大部分非线性的状况。常用的非线性activate function有Sigmoid $f(x) = \frac{1} {1 + e^{-x}}$, ReLU $f(x) = max(0, x)$等。定义损失函数（L, loss function），$L{(w, b)}$用来评估模型计算结果和已知结果的差异，整个训练的过程在于缩小损失函数的差异。假设模型为$y=b+w*x$，损失函数可以定义为$L(b,w)=\frac{1}{N} \sum_{n=1}^{N} e_n$其中$e$代表模型计算的结果$y$和已知的结果$\hat{y}$的差异，可以表示为：$e = |{y-\hat{y}}|$, $L$ 为MAE(Mean Absolute Error)或$e = ({y-\hat{y}})^2$, $L$ 为MSE(Mean Square Error)。训练迭代过程（Optimization），由于我们的目标是寻找Loss function的最小值，使得模型的计算结果最接近training和test data的已知结果，训练过程是先选择一个任意起始点（给定任意的$b$和$w$）,将Loss function对不同的参数（$b$ &amp; $w$）分别做偏微分，如果选定的起始点$w_0$,使用梯度下降方法迭代求Loss function的最小值。需要注意的是，迭代步进的距离$\eta$通常为经验值或是训练过程测试出来，在某些模型，有可能步伐跨大了，错过了最优解（global minima），也有可能步伐跨小了，走不出局部低点（local minima），请参考以下图解及AI解释，帮助理解。机器学习的流程如下图所示：每一轮训练更新模型参数，用更新后的参数再进行下一轮，直到迭代出满意的模型（通过训练和测试数据）,如果无法收敛，则需要修改模型设计并重新训练模型。机器学习模型（贴近实际应用）¶前面提到，简单的线性方程式不足以模拟实际案例，对于二元连续的曲线，我们需要结合多段的曲线来拟合实际的需求。前述的线性方程式$y = b + w*x$ 会转换成如下的矩阵方程式，其中i为用来拟合的曲线数量，j为每条曲线的特征数量，叠加后的结果更接近机器学习适用的模型。将模型的input $x$, bias $b$, weighting $w$ 和 activate function $sigmoid$ 用图形节点表示出来如下图。当叠加更多层时，增加模型的参数变化，模型的拓扑图会如下图。
        </div>
          <a href="/tutorials/1/01/01/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
        
      </div>
    </div>
  </div>
</article>

          </div>
        
          <div class="relative w-100 mb4">
            
<article class="bb b--black-10">
  <div class="db pv4 ph3 ph0-l no-underline dark-gray">
    <div class="flex flex-column flex-row-ns">
      
      <div class="blah w-100">
        <h1 class="f3 fw1 athelas mt0 lh-title">
          <a href="/tutorials/1/01/01/" class="color-inherit dim link">
            
            </a>
        </h1>
        <div class="f6 f5-l lh-copy nested-copy-line-height nested-links">
          <!DOCTYPE html>003_LLM_Basic003 LLM - Basic¶Reference:
Github repo: https://github.com/hkproj/transformer-from-scratch-notesVideo: https://www.youtube.com/watch?v=bCz4OMemCcA什么是LLM?¶大语言模型LLM(Large language model)是能够实现通用语言生成的模型，从大量的文本透过自监督和半监督式训练所得到的神经网路模型。目前最火热的大语言模型是基于Transformer架构来构建的GPT模型，如openai的GPT3.5和GPT4, Google的PaLM和Gemini，Meta的LLaMA（开源模型），以及Anthropic的Claude。LLM可用于文本生成，属于一种生成式AI，通过输入的文本预测下一个标记或是单词。简言之，LLM在做的工作是“文字接龙”，透过前面的叙述，选择最适当的下一个单词。Transformer架构最早在2017年一篇名为“Attention Is All You Need”的论文被提及，演变到现在，除了文本生成的功能，也扩展到图像生成，语音生成等多模态(Multi modal)领域。Transformer架构对比传统用于seq2seq的时间序列或文字序列的模型架构（如RNN，LSTM，CNN）有其优势，Transformer可以同时处理整个序列，利用现代计算机并行计算的能力，加快模型训练，同时自注意力机制，能捕获输入序列中长距离的依赖关系。
Transformer架构¶Transformer架构如下图：一般神经网路常由Encoder和Decoder组成，Transformer架构的左侧部分为Encoder,右侧部分为Decoder。
Encoder¶Transformer的Encoder部分，如下图所示：
Embedding¶Embedding是将原始的输入字串先做Tokenize(分成一个个单词或小于一个单词的组合)，Token数量也常被生成式模型作为计费的标准。再将每个Token所对应的ID标记（每个Token有固定的ID），再将每个Token编码成一个512长的向量（不同模型的embedding长度不同），注意Token的ID是固定的，类似字典的位置，但是Embedding之后的向量内容，在同一个Token的不同字串场景不相同。如下图所示：
Positional Encoding¶Positional Encoding的目的是将每个Token在句子中的位置信息增加到编码当中，使得Token在句子中的位置，不同Token的相邻关系能够被模型在训练的过程学习。首先使用下图中两个方程式来计算每个Token长度为512的向量数值。
再将位置信息和前述Token编码向量相加，做为输入神经网路前处理的编码，如下图：
Self Attention (Single-Head Self Attention)¶Self Attention自注意力机制是个巧妙地安排。
$Attention(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V$
上图中说明自注意力机制的原理，$Q$，$K$，$V$都是相同的输入，图中示例是一个$[6, 512]$的矩阵，代表着6个Token的单词和前述每个Token Embedding后512长的向量。将$Q$和$K^T$矩阵相乘之后再除以$\sqrt{512}$得到$[6, 6]$的矩阵其中包含了每个Token与其他Token的关联性。这里用到的Softmax是机器学习常用在output layer的一个方程式，其目的是使得每个输出的综合介于0~1之间，相当于人为的将输出限制在0~1之间，以便找出数值最大的输出当作预测的结果。
$Softmax(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{K} e^{x_j}}$
接下来再将上图中$[6,6]$的矩阵乘上$V$是原始输入的$[6, 512]$矩阵，所得到的$[6, 512]$自注意力矩阵，这个矩阵的运算包含了每个Token在字典的ID，在句子的位置，以及每个Token和其他Token相对关系。
Multi-Head Attention¶多头注意力机制是Transformer架构最重要的一项设计，其定义如下：
$Multi-Head(Q,K,V)$的定义是将$head_1~head_h$组合起来，再乘上$W^o$, 而$head_i$则是将$(Q，K，V)$分别乘上一组参数$（W_i^Q, W_i^K, W_i^B）$，用前一节的但注意力的算法计算出来。
详细的Multi-Head-Attention计算说明如下图：
$seq$代表输入字串长度（tokens数量），例如前面案例为6个tokens。$d_{model}$代表模型的维度，例如前面案例，使用512的长度做编码。= $h$代表有几个头，positional encoding后，分了四个分支输入给Multi-Head-Attention和Add &amp; Norm，因此$h$数值为4。$d_k = d_v = \frac{d_{model}}{h}$,因此数值为$512/4=128$。上图计算的过程说明：
        </div>
          <a href="/tutorials/1/01/01/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
        
      </div>
    </div>
  </div>
</article>

          </div>
        
      </section>

      
      <section class="w-100">
        <h1 class="f3">More</h1>
        
        
          <h2 class="f5 fw4 mb4 dib mr3">
            <a href="/tutorials/1/01/01/" class="link black dim">
              
            </a>
          </h2>
        

        
        
          <a href="/tutorials/" class="link db f6 pa2 br3 bg-mid-gray white dim w4 tc">All Tutorials</a>
        
        </section>
      

      </div>
  

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://ai-workshop-newsletter.pages.dev/" >
    &copy;  ai-workshop-newsletter 2024 
  </a>
    <div>
<div class="ananke-socials">
  
</div>
</div>
  </div>
</footer>

  </body>
</html>
